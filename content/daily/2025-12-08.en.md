---
title: "Bubble's Brain - 2025-12-08"
date: 2025-12-08T09:00:00+08:00
description: "Bubble's Brain - Dec 8, 2025"
categories:
  - Daily Brief
tags:
  - AI
  - Artificial Intelligence
  - Industry Updates
draft: false
---

## AI News 2025-12-08

> `AI Daily Brief`

### **Summary**

```
Google Gemini 3 can now generate real-time interactive 3D particle systems from plain text, and its CEO predicts AGI may arrive before 2030.
Google also introduced the Titans architecture for ultra-long context, making progress on long-term memory and continual learning.
Separately, the Qwen team published a new view on why RL training for LLMs can be unstable and proposed methods to stabilize training, especially for MoE models.
```

### **Today's AI News**

1. **Gemini 3 can generate interactive 3D particle systems from text, no coding required**: **Google Gemini 3** now supports generating real-time interactive **3D particle systems** from simple text prompts, without programming. Users can even control particle effects with hand gestures captured by a camera. The article contrasts two Gemini tools: **Gemini Canvas**, an integrated real-time rendering environment aimed at quickly producing interactive deliverables for non-developers; and **Google AI Studio**, positioned as a developer “arsenal,” supporting up to **2M tokens** of context and system-level instructions for building and debugging applications.

2. **DeepMind CEO Demis Hassabis: AGI could arrive before 2030; Google unveils “Titans” architecture**: **Demis Hassabis** predicted **AGI** may be achieved **before 2030**, but said it requires one or two “**Transformer-level**” breakthroughs. Meanwhile, at NeurIPS 2025, Google introduced **Titans**, a new **AI architecture** seen as a potential successor to Transformers. Titans combines the fast response of **RNNs** with the power of Transformers and leverages the **MIRAS** theoretical framework to handle **>2M tokens** of ultra-long context, with key advances in **long-term memory** and **continual learning**.

3. **Microsoft open-sources VibeVoice, a voice AI project**: **Microsoft** open-sourced a cutting-edge voice AI project called **VibeVoice** on **GitHub**, which has already gained **12,000+ stars**.

4. **ai-engineering-hub: deep tutorials on LLMs, RAG, and real-world AI agents**: **ai-engineering-hub** is a GitHub project providing in-depth tutorials on **LLMs**, **RAG**, and real-world **AI agent** applications, currently with **21,762 stars**.

5. **claude-quickstarts: Anthropic’s quickstart repository collection for Claude API**: **claude-quickstarts** is Anthropic’s set of quickstart repositories to help developers get started with the **Claude API** and build deployable apps, currently with **11,142 stars**.

6. **Qwen team explains why RL for LLMs can be unstable and proposes stabilization techniques**: The Qwen team published a paper offering a new “**first-order approximation**” view of instability in **LLM reinforcement learning (RL)**. The work argues that token-level objectives can be seen as a first-order approximation of expected sequence-level rewards, and that the approximation’s validity depends on numerical differences between training and inference and on the magnitude of policy updates. This perspective unifies why techniques like **importance sampling** and **clipping** stabilize training. For **Mixture-of-Experts (MoE)** models, the team proposed **Routing Replay** to freeze expert routing and improve stability. Extensive experiments on a **30B MoE** model found that on-policy training is most stable with importance-sampling correction; off-policy training requires both clipping and routing replay to avoid collapse. The paper also reports that once training is stable, different cold-start methods converge to similar final performance—suggesting future work should focus more on RL methods than on cold-start details.

